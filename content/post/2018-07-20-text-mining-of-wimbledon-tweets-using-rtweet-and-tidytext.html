---
title: Text mining of Wimbledon tweets using rtweet and tidytext
author: Jacquie Tran
date: '2018-07-20'
slug: text-mining-of-wimbledon-tweets-using-rtweet-and-tidytext
categories:
  - Off-field analyses
tags:
  - text mining
  - tennis
  - sports analytics
  - r stats
  - tidytext
  - rtweet
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: post
draft: yes
---



<div id="new-job-new-skills" class="section level1">
<h1>New job, new skills</h1>
<p>This year, I started in a new role as Senior Insights Researcher at High Performance Sport New Zealand. For me, one of the big drawcards of this role was the opportunity to learn and grow my skills and competencies as a performance sport professional.</p>
<p>Our team regularly deals with qualitative data, so as an applied sport scientist with a strongly quantitative training, it has been a steep learning curve but also enjoyable to open my mind up to different ways of thinking and knowing.</p>
<p>Online discussions about sports analytics seem heavily focused on numbers and what we can do with quantitative data. I remain just as interested in these discussions as I was before starting my current job, but I also see opportunities for more of us in the sports analytics community to think about what else we can learn by capturing and analysing qualitative data. In that spirit, this post is a brief glimpse at what is possible when applying text mining methods.</p>
</div>
<div id="getting-the-data" class="section level1">
<h1>Getting the data</h1>
<p>I’ve been thinking about writing this post for a couple of months now, and every so often I trawl the internet looking for text corpuses that are relevant to sport - so far, my efforts have turned up fairly light results. (If anyone knowledgeable in this domain can point me to relevant open data sets, then <a href="https://www.twitter.com/jacquietran">please get in touch!</a>)</p>
<p>So…it’s <code>rtweet</code> to the rescue!</p>
<blockquote>
<code>rtweet</code> is a slick package by <a href="https://www.twitter.com/kearneymw">Mike Kearney</a> (<span class="citation">@kearneymw</span>) for downloading tweets and associated metadata. The package website is a great place to start if you want to give it a crack: <a href="https://rtweet.info/">https://rtweet.info/</a>
</blockquote>
<p>When I started this blog post, Wimbledon 2018 was in its final days with the men’s singles playing quarter finals and the women’s singles playing semi finals. I used <a href="https://github.com/jacquietran/2018_wimbledon_tweets/blob/master/extract_wimby_tweets.R">this script</a> to download tweets that:</p>
<ul>
<li>Included the keywords <em>‘federer’</em> or <em>‘serena williams’</em> (two separate searches),</li>
<li>Were written in English, and</li>
<li>Did not include retweets.</li>
</ul>
<p>All I need to do here is load the libraries I need for this analysis and read the data sets into R from the CSV files stored in <a href="https://github.com/jacquietran/2018_wimbledon_tweets">this Github repo</a>.</p>
<pre class="r"><code># Load libraries
library(readr)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(tidytext)</code></pre>
<pre class="r"><code># Read data into R
fed &lt;- read_delim(
    &quot;https://raw.githubusercontent.com/jacquietran/2018_wimbledon_tweets/master/fed.csv&quot;,
    delim = &quot;,&quot;, col_names = TRUE, col_types = NULL)

serena &lt;- read_delim(
    &quot;https://raw.githubusercontent.com/jacquietran/2018_wimbledon_tweets/master/serena.csv&quot;,
    delim = &quot;,&quot;, col_names = TRUE, col_types = NULL)</code></pre>
</div>
<div id="examining-and-cleaning-the-data" class="section level1">
<h1>Examining and cleaning the data</h1>
<p>If you visually scan through the data set, you’ll see it has several variables. For the purposes of this post, I am most interested in the following:</p>
<ul>
<li><code>text</code>: The text content of a tweet.</li>
<li><code>favorite_count</code>: The number of times a tweet has been ‘favourited’ (or in modern Twitter parlance, ‘liked’…ugh) by other user accounts</li>
<li><code>retweet_count</code>: The number of times a tweet has been ‘retweeted’ (shared) by other user accounts</li>
<li><code>followers_count</code>: The number of user accounts that follow the account who posted a tweet</li>
</ul>
<blockquote>
Not familiar with Twitter? Here’s a quick primer that will help you get a lay of the land: <a href="http://www.momthisishowtwitterworks.com/">Mom, This is How Twitter Works</a>
</blockquote>
<p>We have our data and we know what variables we’re going to focus on. How do we get from raw tweet data to text mining?</p>
<p>Enter: the <code>tidytext</code> package.</p>
<ul>
<li>Introduce <code>tidytext</code> package, point to book and tutorial(s)</li>
<li>Fix text encoding if needed / remove emojis</li>
<li>Remove stop words</li>
<li>Note duplicate tweets</li>
</ul>
<pre class="r"><code># Remove URLs using regex
fed$text &lt;- gsub(&quot;http[[:alnum:][:punct:]]*&quot;, &quot;&quot;, fed$text)
serena$text &lt;- gsub(&quot;http[[:alnum:][:punct:]]*&quot;, &quot;&quot;, serena$text)
# Ref: http://www.rdatamining.com/books/rdm/faq/removeurlsfromtext

# Remove emojis
fed$text &lt;- gsub(&quot;[^\x01-\x7F]&quot;, &quot;&quot;, fed$text)
serena$text &lt;- gsub(&quot;[^\x01-\x7F]&quot;, &quot;&quot;, serena$text)
# Ref: https://stackoverflow.com/questions/44893354/remove-emoticons-in-r-using-tm-package

# Unnest tokens using tidytext
fed_tidy &lt;- fed %&gt;%
    select(user_id, status_id, screen_name,
                 favorite_count, retweet_count, followers_count, text) %&gt;%
    unnest_tokens(word, text) %&gt;%
    anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>serena_tidy &lt;- serena %&gt;%
    select(user_id, status_id, screen_name,
                 favorite_count, retweet_count, followers_count, text) %&gt;%
    unnest_tokens(word, text) %&gt;%
    anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code># Count most frequently occurring words
fed_tidy %&gt;% count(word, sort = TRUE) %&gt;% filter(!word %in% c(&quot;federer&quot;, &quot;wimbledon&quot;))</code></pre>
<pre><code>## # A tibble: 14,791 x 2
##    word         n
##    &lt;chr&gt;    &lt;int&gt;
##  1 anderson  9746
##  2 roger     6596
##  3 kevin     4781
##  4 match     4126
##  5 nadal     2893
##  6 england   2670
##  7 set       2503
##  8 2         2322
##  9 sets      2191
## 10 lost      2084
## # ... with 14,781 more rows</code></pre>
</div>
<div id="n-gram-frequency" class="section level1">
<h1>n-gram frequency</h1>
</div>
<div id="sentiment-analysis" class="section level1">
<h1>Sentiment analysis</h1>
<ul>
<li>bing (positive / negative binary categorisation)</li>
<li>AFINN (positive to negative, continuous scale)</li>
<li>nrc (multiple sentiment categories)</li>
</ul>
</div>
<div id="correlating-sentiment-to-favourites-and-retweets" class="section level1">
<h1>Correlating sentiment to favourites and retweets</h1>
</div>
<div id="what-else-can-text-mining-do" class="section level1">
<h1>What else can text mining do?</h1>
<ul>
<li>Sentiment analysis at the sentence level</li>
<li>n-gram network analysis</li>
<li>Parts-of-speech tagging</li>
<li>Topic modelling</li>
</ul>
</div>
